[
  { "testcase": "[Conformance]" },
  { "testcase": "[k8s.io] [Feature:Example] [k8s.io] Downward API should create a pod that prints his name and namespace" },
  { "testcase": "[k8s.io] [Feature:Example] [k8s.io] Liveness liveness pods should be automatically restarted" },
  { "testcase": "[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s." },
  { "testcase": "[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support pod.Spec.SecurityContext.SupplementalGroups" },
  { "testcase": "[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha runtime/default annotation [Feature:Seccomp]" },
  { "testcase": "[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha unconfined annotation on the container [Feature:Seccomp]" },
  { "testcase": "[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha unconfined annotation on the pod [Feature:Seccomp]" },
  { "testcase": "[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp default which is unconfined [Feature:Seccomp]" },
  { "testcase": "[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [NodeConformance]" },
  { "testcase": "[k8s.io] Sysctls [NodeFeature:Sysctls] should not launch unsafe, but not explicitly enabled sysctls on the node" },
  { "testcase": "[k8s.io] Sysctls [NodeFeature:Sysctls] should reject invalid sysctls" },
  { "testcase": "[k8s.io] Sysctls [NodeFeature:Sysctls] should support sysctls" },
  { "testcase": "[k8s.io] Sysctls [NodeFeature:Sysctls] should support unsafe sysctls which are actually whitelisted" },
  { "testcase": "[sig-api-machinery] Initializers [Feature:Initializers] should be invisible to controllers by default" },
  { "testcase": "[sig-api-machinery] Initializers [Feature:Initializers] will be set to nil if a patch removes the last pending initializer" },
  { "testcase": "[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods" },
  { "testcase": "[sig-auth] [Feature:NodeAuthorizer] A node shouldn't be able to create another node" },
  { "testcase": "[sig-auth] [Feature:NodeAuthorizer] A node shouldn't be able to delete another node" },
  { "testcase": "[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent configmap should exit with the Forbidden error, not a NotFound error" },
  { "testcase": "[sig-auth] [Feature:NodeAuthorizer] Getting a non-existent secret should exit with the Forbidden error, not a NotFound error" },
  { "testcase": "[sig-auth] [Feature:NodeAuthorizer] Getting a secret for a workload the node has access to should succeed" },
  { "testcase": "[sig-auth] [Feature:NodeAuthorizer] Getting an existing configmap should exit with the Forbidden error" },
  { "testcase": "[sig-auth] [Feature:NodeAuthorizer] Getting an existing secret should exit with the Forbidden error" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl apply apply set/view last-applied" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl apply should apply a new configuration to an existing RC" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl apply should reuse port when apply to an existing SVC" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info dump should check if cluster-info dump succeeds" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl copy should copy a file from a running Pod" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota with scopes" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota without scopes" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl create quota should reject quota with invalid scopes" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Kubectl run CronJob should create a CronJob" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path  [Conformance]" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Simple pod should handle in-cluster config" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Simple pod should return command exit codes" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Simple pod should support exec" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through an HTTP proxy" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through kubectl proxy" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Simple pod should support inline execution and attach" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Simple pod should support port-forward" },
  { "testcase": "[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects" },
  { "testcase": "[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects" },
  { "testcase": "[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects" },
  { "testcase": "[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 should support forwarding over websockets" },
  { "testcase": "[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects" },
  { "testcase": "[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects" },
  { "testcase": "[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects" },
  { "testcase": "[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost should support forwarding over websockets" },
  { "testcase": "[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the token secret when the secret expired" },
  { "testcase": "[sig-cluster-lifecycle] [Feature:BootstrapTokens] should not delete the token secret when the secret is not expired" },
  { "testcase": "[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to cadvisor port 4194 using proxy subresource" },
  { "testcase": "[sig-cluster-lifecycle] Ports Security Check [Feature:KubeletSecurity] should not be able to proxy to the readonly kubelet port 10255 using proxy subresource" },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's multiple priority class scope (quota set to pod count: 2) against 2 pods with same priority classes." },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (cpu, memory quota set) against a pod with same priority class." },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against 2 pods with different priority class." },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against 2 pods with same priority class." },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpExists)." },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against a pod with different priority class (ScopeSelectorOpNotIn)." },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:PodPriority] should verify ResourceQuota's priority class scope (quota set to pod count: 1) against a pod with same priority class." },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with best effort scope using scope-selectors." },
  { "testcase": "[sig-scheduling] ResourceQuota [Feature:ScopeSelectors] should verify ResourceQuota with terminating scopes through scope selectors." },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim with a storage class. [sig-storage]" },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim. [sig-storage]" },
  { "testcase": "[sig-storage] ConfigMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] ConfigMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] Downward API volume should provide podname as non-root with fsgroup [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] Downward API volume should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] Dynamic Provisioning [k8s.io] GlusterDynamicProvisioner should create and delete persistent volumes [fast]" },
  { "testcase": "[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] files with FSGroup ownership should support (root,0644,tmpfs)" },
  { "testcase": "[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is non-root" },
  { "testcase": "[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is root" },
  { "testcase": "[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] nonexistent volume subPath should have the correct mode and owner using FSGroup" },
  { "testcase": "[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on default medium should have the correct mode using FSGroup" },
  { "testcase": "[sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on tmpfs should have the correct mode using FSGroup" },
  { "testcase": "[sig-storage] HostPath should support r/w [NodeConformance]" },
  { "testcase": "[sig-storage] HostPath should support subPath [NodeConformance]" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and write from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set fsGroup for one pod" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set fsGroup for one pod" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set fsGroup for one pod" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set fsGroup for one pod" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and write from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set fsGroup for one pod" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and write from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set fsGroup for one pod" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and write from pod1" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set fsGroup for one pod" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" },
  { "testcase": "[sig-storage] PersistentVolumes-local [Volume type: blockfswithformat] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously" },
  { "testcase": "[sig-storage] PersistentVolumes-local [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1" },
  { "testcase": "[sig-storage] Projected configMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] Projected configMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]" },
  { "testcase": "[sig-storage] Projected secret should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from docker hub [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from gcr.io [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull from private registry without secret [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull image from invalid registry [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull non-existing image from gcr.io [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message from log output if TerminationMessagePolicy FallbackToLogOnError is set [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message if TerminationMessagePath is set [NodeConformance]" },
  { "testcase": "[k8s.io] Container Runtime blackbox test when starting a container that exits should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance]" },
  { "testcase": "[k8s.io] PrivilegedPod [NodeConformance] should enable privileged commands" },
  { "testcase": "[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 0 [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when true [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [NodeConformance]" },
  { "testcase": "[sig-api-machinery] CustomResourceDefinition Watch CustomResourceDefinition Watch watch on custom resource definition objects" },
  { "testcase": "[sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob" },
  { "testcase": "[sig-api-machinery] Garbage collector should orphan pods created by rc if deleteOptions.OrphanDependents is nil" },
  { "testcase": "[sig-api-machinery] Garbage collector should support cascading deletion of custom resources" },
  { "testcase": "[sig-api-machinery] Garbage collector should support orphan deletion of custom resources" },
  { "testcase": "[sig-api-machinery] Generated clientset should create pods, set the deletionTimestamp and deletionGracePeriodSeconds of the pod" },
  { "testcase": "[sig-api-machinery] Generated clientset should create v1beta1 cronJobs, delete cronJobs, watch cronJobs" },
  { "testcase": "[sig-api-machinery] Secrets should fail to create secret in volume due to empty secret key" },
  { "testcase": "[sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls" },
  { "testcase": "[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata" },
  { "testcase": "[sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls" },
  { "testcase": "[sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes" },
  { "testcase": "[sig-api-machinery] Servers with support for Table transformation should return pod details" },
  { "testcase": "[sig-api-machinery] Watchers should receive events on concurrent watches in same order" },
  { "testcase": "[sig-apps] CronJob should delete successful finished jobs with limit of one successful job" },
  { "testcase": "[sig-apps] CronJob should not emit unexpected warnings" },
  { "testcase": "[sig-apps] CronJob should remove from active list jobs that have been deleted" },
  { "testcase": "[sig-apps] CronJob should replace jobs when ReplaceConcurrent" },
  { "testcase": "[sig-apps] CronJob should schedule multiple jobs concurrently" },
  { "testcase": "[sig-apps] Deployment deployment reaping should cascade to its replica sets and pods" },
  { "testcase": "[sig-apps] Deployment deployment should support rollback" },
  { "testcase": "[sig-apps] Deployment iterative rollouts should eventually progress" },
  { "testcase": "[sig-apps] Deployment test Deployment ReplicaSet orphaning and adoption regarding controllerRef" },
  { "testcase": "[sig-apps] DisruptionController evictions: enough pods, absolute => should allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage => should allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage => should allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer => should not allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: no PDB => should allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: too few pods, absolute => should not allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage => should not allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController should create a PodDisruptionBudget" },
  { "testcase": "[sig-apps] DisruptionController should update PodDisruptionBudget status" },
  { "testcase": "[sig-apps] Job should adopt matching orphans and release non-matching pods" },
  { "testcase": "[sig-apps] Job should delete a job" },
  { "testcase": "[sig-apps] Job should exceed active deadline" },
  { "testcase": "[sig-apps] Job should exceed backoffLimit" },
  { "testcase": "[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted" },
  { "testcase": "[sig-apps] Job should run a job to completion when tasks sometimes fail and are not locally restarted" },
  { "testcase": "[sig-apps] Job should run a job to completion when tasks succeed" },
  { "testcase": "[sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota" },
  { "testcase": "[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota" },
  { "testcase": "[sig-auth] Certificates API should support building a client with a CSR" },
  { "testcase": "[sig-auth] PodSecurityPolicy should forbid pod creation when no PSP is available" },
  { "testcase": "[sig-auth] ServiceAccounts should ensure a single API token exists" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path  [Conformance]" },
  { "testcase": "[sig-instrumentation] Cadvisor should be healthy on every node." },
  { "testcase": "[sig-instrumentation] MetricsGrabber should grab all metrics from API server." },
  { "testcase": "[sig-instrumentation] MetricsGrabber should grab all metrics from a ControllerManager." },
  { "testcase": "[sig-instrumentation] MetricsGrabber should grab all metrics from a Kubelet." },
  { "testcase": "[sig-instrumentation] MetricsGrabber should grab all metrics from a Scheduler." },
  { "testcase": "[sig-network] DNS should provide DNS for pods for Hostname and Subdomain" },
  { "testcase": "[sig-network] DNS should support configurable pod resolv.conf" },
  { "testcase": "[sig-network] NetworkPolicy NetworkPolicy between server and client should allow ingress access on one named port [Feature:NetworkPolicy]" },
  { "testcase": "[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce multiple, stacked policies with overlapping podSelectors [Feature:NetworkPolicy]" },
  { "testcase": "[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on NamespaceSelector [Feature:NetworkPolicy]" },
  { "testcase": "[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on PodSelector [Feature:NetworkPolicy]" },
  { "testcase": "[sig-network] NetworkPolicy NetworkPolicy between server and client should enforce policy based on Ports [Feature:NetworkPolicy]" },
  { "testcase": "[sig-network] NetworkPolicy NetworkPolicy between server and client should support a 'default-deny' policy [Feature:NetworkPolicy]" },
  { "testcase": "[sig-network] NetworkPolicy NetworkPolicy between server and client should support allow-all policy [Feature:NetworkPolicy]" },
  { "testcase": "[sig-network] Networking should provide unchanging, static URL paths for kubernetes api services" },
  { "testcase": "[sig-network] Services should be able to change the type from ClusterIP to ExternalName" },
  { "testcase": "[sig-network] Services should be able to change the type from ExternalName to ClusterIP" },
  { "testcase": "[sig-network] Services should be able to change the type from ExternalName to NodePort" },
  { "testcase": "[sig-network] Services should be able to change the type from NodePort to ExternalName" },
  { "testcase": "[sig-network] Services should be able to switch session affinity for NodePort service" },
  { "testcase": "[sig-network] Services should be able to switch session affinity for service with type clusterIP" },
  { "testcase": "[sig-network] Services should be able to update NodePorts with two same port numbers but different protocols" },
  { "testcase": "[sig-network] Services should check NodePort out-of-range" },
  { "testcase": "[sig-network] Services should create endpoints for unready pods" },
  { "testcase": "[sig-network] Services should have session affinity work for NodePort service" },
  { "testcase": "[sig-network] Services should have session affinity work for service with type clusterIP" },
  { "testcase": "[sig-network] Services should preserve source pod IP for traffic thru service cluster IP" },
  { "testcase": "[sig-network] Services should prevent NodePort collisions" },
  { "testcase": "[sig-network] Services should release NodePorts on delete" },
  { "testcase": "[sig-network] Services should use same NodePort with same port but different protocols" },
  { "testcase": "[sig-node] ConfigMap should fail to create configMap in volume due to empty configmap key" },
  { "testcase": "[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied." },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a configMap." },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a pod." },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a replica set." },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a replication controller." },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a secret." },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a service." },
  { "testcase": "[sig-scheduling] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated." },
  { "testcase": "[sig-scheduling] ResourceQuota should verify ResourceQuota with best effort scope." },
  { "testcase": "[sig-scheduling] ResourceQuota should verify ResourceQuota with terminating scopes." },
  { "testcase": "[sig-storage] Dynamic Provisioning DynamicProvisioner allowedTopologies should create persistent volume in the zone specified in allowedTopologies of storageclass" },
  { "testcase": "[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : configmap" },
  { "testcase": "[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : projected" },
  { "testcase": "[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : secret" },
  { "testcase": "[sig-storage] PV Protection Verify that PV bound to a PVC is not removed immediately" },
  { "testcase": "[sig-storage] PVC Protection Verify that PVC in active use by a pod is not removed immediately" },
  { "testcase": "[sig-storage] PVC Protection Verify that scheduling of a pod that uses PVC that is being deleted fails and the pod becomes Unschedulable" },
  { "testcase": "[sig-storage] PersistentVolumes NFS when invoking the Recycle reclaim policy should test that a PV becomes Available and is clean after the PVC is deleted." },
  { "testcase": "[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PV and a pre-bound PVC: test write access" },
  { "testcase": "[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and a pre-bound PV: test write access" },
  { "testcase": "[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and non-pre-bound PV: test write access" },
  { "testcase": "[sig-storage] PersistentVolumes NFS with Single PV - PVC pairs should create a non-pre-bound PV and PVC: test write access" },
  { "testcase": "[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 2 PVs and 4 PVCs: test write access" },
  { "testcase": "[sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 3 PVs and 3 PVCs: test write access" },
  { "testcase": "[sig-storage] PersistentVolumes-local  Pod with node different from PV's NodeAffinity should fail scheduling due to different NodeAffinity" },
  { "testcase": "[sig-storage] PersistentVolumes-local  Pod with node different from PV's NodeAffinity should fail scheduling due to different NodeSelector" },
  { "testcase": "[sig-storage] Volumes ConfigMap should be mountable" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount" },
  { "testcase": "[sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should be mountable" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount" },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should be mountable" },
  { "testcase": "[k8s.io] Pods should support pod readiness gates [NodeFeature:PodReadinessGate]" },
  { "testcase": "[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance]" },
  { "testcase": "[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [NodeConformance]" },
  { "testcase": "[sig-apps] DisruptionController evictions: enough pods, absolute => should allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage => should allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage => should allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer => should not allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: no PDB => should allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: too few pods, absolute => should not allow an eviction" },
  { "testcase": "[sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage => should not allow an eviction" },
  { "testcase": "[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path [Conformance]" },
  { "testcase": "[sig-network] Networking should provide Internet connection for containers [Feature:Networking-IPv4]", "exclude": [ "azure" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing directory", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support existing single file", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Inline-volume (default fs)] volumes should be mountable", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (default fs)] volumes should be mountable", "only": [ "openstack" ] },
  { "testcase": "[sig-storage] In-tree Volumes [Driver: ceph][Feature:Volumes] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources", "only": [ "openstack" ] }
]